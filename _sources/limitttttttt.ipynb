{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTIfyHSsW37u9Btq5rY7k1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sastrawi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOL0tCcrhX5Z","executionInfo":{"status":"ok","timestamp":1729135758713,"user_tz":-420,"elapsed":10580,"user":{"displayName":"22-052 Yudha nuur cahyo","userId":"02966233732175847645"}},"outputId":"7e01e58d-ed3b-44b8-a4c1-2e8d39c4b217"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sastrawi\n","  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n","Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/209.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sastrawi\n","Successfully installed sastrawi-1.0.1\n"]}]},{"cell_type":"code","source":["kategori_list = [\n","    {\"url\": \"https://nasional.tempo.co/\"},\n","    {\"url\": \"https://olahraga.tempo.co/\"},\n","    {\"url\": \"https://dunia.tempo.co/\"},\n","    {\"url\": \"https://nasional.tempo.co/\"},\n","    {\"url\": \"https://metro.tempo.co/\"}\n","]"],"metadata":{"id":"b8rI7-0gi59F"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"kpDKxr0ChHp0","executionInfo":{"status":"error","timestamp":1729136171222,"user_tz":-420,"elapsed":1945,"user":{"displayName":"22-052 Yudha nuur cahyo","userId":"02966233732175847645"}},"outputId":"0345a789-29fd-4b29-a5f8-92f15accc6ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Scraping https://www.tempo.co/kanal/ekonomi ...\n","Failed to retrieve page content from https://www.tempo.co/kanal/ekonomi/?page=1\n","No content found on page 1. Stopping.\n","Scraping https://www.tempo.co/kanal/olahraga ...\n","Failed to retrieve page content from https://www.tempo.co/kanal/olahraga/?page=1\n","No content found on page 1. Stopping.\n","Scraping https://www.tempo.co/kanal/dunia ...\n","Failed to retrieve page content from https://www.tempo.co/kanal/dunia/?page=1\n","No content found on page 1. Stopping.\n","Scraping https://www.tempo.co/kanal/nasional ...\n","Failed to retrieve page content from https://www.tempo.co/kanal/nasional/?page=1\n","No content found on page 1. Stopping.\n","Scraping https://www.tempo.co/kanal/metro ...\n","Failed to retrieve page content from https://www.tempo.co/kanal/metro/?page=1\n","No content found on page 1. Stopping.\n","No articles were scraped. Please check the URLs and try again.\n"]},{"output_type":"error","ename":"KeyError","evalue":"'lower case'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1bd42577e6a6>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mcasefolding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_acak\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lower case'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_punct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'lower case'"]}],"source":["# -*- coding: utf-8 -*-\n","import pandas as pd\n","import re\n","import requests\n","from tqdm import tqdm\n","import time\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","# Fungsi untuk mengambil konten halaman dengan BeautifulSoup\n","from bs4 import BeautifulSoup\n","\n","def get_page_content(url):\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n","    try:\n","        response = requests.get(url, headers=headers, timeout=10)\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","            return soup\n","        else:\n","            print(f\"Failed to retrieve page content from {url}\")\n","            return None\n","    except requests.exceptions.Timeout:\n","        print(f\"Request to {url} timed out.\")\n","        return None\n","    except Exception as e:\n","        print(f\"Error during requests to {url} : {e}\")\n","        return None\n","\n","def extract_article_content(soup):\n","    try:\n","        title = soup.find('h1').get_text().strip() if soup.find('h1') else \"\"\n","        date = soup.find('p', class_=\"pt-20 date\").get_text().strip() if soup.find('p', class_=\"pt-20 date\") else \"\"\n","        content_elements = soup.find_all('p')\n","        content = \" \".join(p.get_text().strip() for p in content_elements)\n","        kategori = soup.find('p', class_=\"breadcrumb-content\").get_text().strip() if soup.find('p', class_=\"breadcrumb-content\") else \"\"\n","        return {\n","            \"Title\": title,\n","            \"Date\": date,\n","            \"Content\": content,\n","            \"Category\": kategori\n","        }\n","    except Exception as e:\n","        print(f\"Error extracting article content: {e}\")\n","        return None\n","\n","def extract_category_data(soup):\n","    articles = soup.find_all('div', class_='text')\n","    article_urls = [article.find('a')['href'] for article in articles if article.find('a')]\n","    return article_urls\n","\n","def scrape_category_page(main_url, max_articles=4):\n","    all_article_data = []\n","    page_num = 1  # Mulai dari halaman pertama\n","\n","    while len(all_article_data) < max_articles:\n","        current_url = f\"{main_url}/?page={page_num}\"\n","        soup = get_page_content(current_url)\n","\n","        if not soup:\n","            print(f\"No content found on page {page_num}. Stopping.\")\n","            break\n","\n","        article_urls = extract_category_data(soup)\n","\n","        if not article_urls:\n","            print(f\"No more articles found on page {page_num}. Stopping.\")\n","            break\n","\n","        for url in article_urls:\n","            if len(all_article_data) >= max_articles:\n","                break\n","\n","            article_soup = get_page_content(url)\n","            if article_soup:\n","                article_data = extract_article_content(article_soup)\n","                if article_data:\n","                    all_article_data.append(article_data)\n","\n","        page_num += 1\n","        time.sleep(2)  # Waktu tunggu tambahan untuk memastikan halaman dimuat\n","\n","    return all_article_data\n","\n","\n","kategori_list = [\n","    {\"url\": \"https://www.tempo.co/kanal/ekonomi\"},\n","    {\"url\": \"https://www.tempo.co/kanal/olahraga\"},\n","    {\"url\": \"https://www.tempo.co/kanal/dunia\"},\n","    {\"url\": \"https://www.tempo.co/kanal/nasional\"},\n","    {\"url\": \"https://www.tempo.co/kanal/metro\"}\n","]\n","\n","all_data = []\n","\n","for category in kategori_list:\n","    print(f\"Scraping {category['url']} ...\")\n","    category_data = scrape_category_page(category[\"url\"], max_articles=4)\n","    all_data.extend(category_data)\n","\n","df = pd.DataFrame(all_data)\n","\n","# Menangani situasi di mana tidak ada artikel yang diambil\n","if df.empty:\n","    print(\"No articles were scraped. Please check the URLs and try again.\")\n","else:\n","    print(df)\n","\n","    df_acak = df.sample(frac=1).reset_index(drop=True)\n","\n","    # Proses Text Preprocessing\n","    def clean_lower(text):\n","        if isinstance(text, str):\n","            return text.lower()\n","        return text\n","\n","    df_acak['lower case'] = df_acak['Content'].apply(clean_lower)\n","\n","    # Lanjutkan dengan kode preprocessing dan model Anda\n","    ...\n","\n","\n","casefolding = pd.DataFrame(df_acak['lower case'])\n","\n","def clean_punct(text):\n","    if isinstance(text, str):\n","        clean_patterns = re.compile(r'[0-9]|[/(){}\\[\\]\\|@,;_]|[^a-z ]')\n","        text = clean_patterns.sub(' ', text)\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","        return text\n","    return text\n","\n","df_acak['tanda baca'] = df_acak['lower case'].apply(clean_punct)\n","\n","def _normalize_whitespace(text):\n","    if isinstance(text, str):\n","        corrected = re.sub(r'\\s+', ' ', text)\n","        return corrected.strip()\n","    return text\n","\n","df_acak['spasi'] = df_acak['tanda baca'].apply(_normalize_whitespace)\n","\n","def clean_stopwords(text):\n","    if isinstance(text, str):\n","        stopword = set(stopwords.words('indonesian'))\n","        text = ' '.join(word for word in text.split() if word not in stopword)\n","        return text.strip()\n","    return text\n","\n","df_acak['stopwords'] = df_acak['spasi'].apply(clean_stopwords)\n","\n","def sastrawistemmer(text):\n","    factory = StemmerFactory()\n","    st = factory.create_stemmer()\n","    text = ' '.join(st.stem(word) for word in tqdm(text.split()) if word in text)\n","    return text\n","\n","df_acak['stemming'] = df_acak['stopwords'].apply(sastrawistemmer)\n","\n","count_vectorizer = CountVectorizer()\n","\n","if 'stemming' in df_acak.columns:\n","    corpus = df_acak['stemming'].tolist()\n","else:\n","    raise KeyError(\"Kolom 'stemming' tidak ada di DataFrame\")\n","\n","x_count = count_vectorizer.fit_transform(corpus)\n","feature_names = count_vectorizer.get_feature_names_out()\n","x_count_df = pd.DataFrame(x_count.toarray(), columns=feature_names)\n","\n","print(x_count_df)\n","\n","# Load the saved model from file\n","filename = 'tfidf_vectorizer.sav'\n","tfidf_vectorizer = pickle.load(open(filename, 'rb'))\n","\n","corpus = df_acak['stemming'].tolist()\n","x_tfidf = tfidf_vectorizer.transform(corpus)\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","tfidf_df = pd.DataFrame(x_tfidf.toarray(), columns=feature_names)\n","cat_df = df_acak[\"Category\"]\n","tfidf_df['Category'] = cat_df.values\n","tfidf_df = tfidf_df[['Category'] + [col for col in tfidf_df.columns if col != 'Category']]\n","\n","print(tfidf_df)\n","\n","# menggunakan label_encoder untuk merubah kata menjadi angka\n","label_encoder = preprocessing.LabelEncoder()\n","tfidf_df['Category'] = label_encoder.fit_transform(tfidf_df['Category'])\n","\n","print(tfidf_df)\n","\n","# Load the saved model from file\n","filename = 'lr_model.sav'\n","lr_model = pickle.load(open(filename, 'rb'))\n","\n","y_test = tfidf_df['Category']\n","x_test = tfidf_df.drop(['Category'], axis=1)\n","y_pred = lr_model.predict(x_test)\n","\n","print(y_pred)\n","\n","# melihat nilai actual dan predicted\n","a = pd.DataFrame({'Actual value': y_test, 'Predicted value':y_pred})\n","print(a)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"WqbeXzkMhO_T","executionInfo":{"status":"ok","timestamp":1729135723016,"user_tz":-420,"elapsed":567,"user":{"displayName":"22-052 Yudha nuur cahyo","userId":"02966233732175847645"}}},"execution_count":null,"outputs":[]}]}